{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Recogition"
      ],
      "metadata": {
        "id": "_dLQvyqIua4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IFGt21YIj5CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226431cc-1e7c-4080-bb4e-da8db2c6d898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision # load datasets\n",
        "import torchvision.transforms as transforms # transform data\n",
        "import torch.nn as nn # basic building block for neural networks\n",
        "import torch.nn.functional as F # import convolution functions like Relu\n",
        "import torch.optim as optim # optimzer\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT_DIR='/content/drive/MyDrive/College/Research/Linh_2023_Research'\n",
        "sys.path.append(ROOT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pZqR6gbjd4A"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGQvCIW1ALhG"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "    data: is a list of tuples with (example, label, length)\n",
        "            where 'example' is a tensor of arbitrary shape\n",
        "            and label/length are scalars\n",
        "    \"\"\"\n",
        "    _, labels, lengths = zip(*data)     # ([batch_size, num_channels, partsCount, 32, 128], [label])\n",
        "    max_len = max(lengths)\n",
        "    n_ftrs = data[0][0].size(1)\n",
        "    features = torch.zeros((len(data), max_len, n_ftrs))\n",
        "    labels = torch.tensor(labels)\n",
        "    lengths = torch.tensor(lengths)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        j, k = data[i][0].size(0), data[i][0].size(1)\n",
        "        features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
        "\n",
        "    return features.float(), labels.long(), lengths.long()\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "      axes[c].set_ylim(ylim)\n",
        "  # figure.suptitle(title)\n",
        "  # plt.show(block=False)\n",
        "  plt.show()\n",
        "\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title('Spectrogram (db)' if not title else title)\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  # plt.show(block=False)\n",
        "  # plt.savefig(title, bbox_inches='tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bcEd9KjgQp"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Data**: The dataset is located in a specified directory (`root_path`). A list of classes (`includes`) and the number of classes (`num_classes`) are provided as arguments when creating an instance of the `SpeakerData` class.\n",
        "\n",
        "**Initializing the Dataset**: The __init__ method initializes the dataset by setting the root_path, generating samples, and loading labels.\n",
        "\n",
        "**Generating Samples** (`make_samples` method):\n",
        "- Randomly selects classes from the dataset if the requested number of classes exceeds the available ones.\n",
        "- Collects file paths and their corresponding labels.\n",
        "\n",
        "**Loading Audio Data** (`load_item` method): Loads and returns the audio waveform, sample rate, and label.\n",
        "\n",
        "**Spectrogram Computation**: Computes the Short-Time Fourier Transform (STFT) of the audio data to generate a spectrogram (`spec`).\n",
        "\n",
        "**Transformations**: Resizes the spectrogram to a fixed size of (32, 128). Transposes the spectrogram tensor.\n",
        "\n",
        "**Extracting Parts**: Divides the spectrogram into multiple parts of size (`image_width`, `n_fft`) if the audio length exceeds `image_width`.\n",
        "\n",
        "**Label Mapping**: Maps the labels to integer indices using label2idx.\n",
        "\n",
        "**Returning Data**: Returns the parts and their corresponding label indices."
      ],
      "metadata": {
        "id": "RKnr-HToDgZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goc6heVslkMi"
      },
      "outputs": [],
      "source": [
        "# Spectrogram parameters\n",
        "n_fft = 512\n",
        "image_width = 64\n",
        "frame_length = 512\n",
        "step_length = 0.001\n",
        "\n",
        "class SpeakerData(Dataset):\n",
        "    \"\"\"Create a Dataset for Speech Commands.\n",
        "\n",
        "    Args:\n",
        "        root (str): Path to the directory where the dataset is found or downloaded.\n",
        "    \"\"\"\n",
        "    def __init__(self, root: str, num_classes: int, includes=[]) -> None:\n",
        "        super().__init__()\n",
        "        self.root_path = root\n",
        "        self.samples = self.make_samples(num_classes, includes)\n",
        "        self.labels, self.label2idx = self.load_labels(self.samples)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def make_samples(self, num_classes: int, classes: list) -> list[Tuple[str, int]]:\n",
        "        \"\"\"Create samples for imported dataset.\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n",
        "        \"\"\"\n",
        "        n = len(classes)\n",
        "        if num_classes - n >= 0:\n",
        "            num_classes -= n\n",
        "            classes.extend(random.sample(os.listdir(self.root_path), num_classes))\n",
        "\n",
        "        paths = []\n",
        "        # for p in Path(self.root_path).glob(\"*/*/*.flac\"):\n",
        "        for p in Path(self.root_path).glob(\"*/*.wav\"):\n",
        "            label = str(p).split(\"/\")[-2]\n",
        "            if label in classes:\n",
        "                paths.append((str(p), label))\n",
        "        paths.sort()\n",
        "        return paths\n",
        "\n",
        "    def _load_item(self, filepath: str, path: str) -> Tuple[Tensor, int, int]:\n",
        "        relpath = os.path.relpath(filepath, path)\n",
        "        label = relpath.split(\"/\")[0]\n",
        "        # Load audio\n",
        "        waveform, sample_rate = torchaudio.load(filepath)\n",
        "        return waveform, sample_rate, label\n",
        "\n",
        "    def load_labels(self, samples: list):\n",
        "        labels = []\n",
        "        label2idx = {}\n",
        "        cur = -1\n",
        "        count = 0\n",
        "        for n in samples:\n",
        "            if cur != n[1]:\n",
        "              labels.append(n[1])\n",
        "              label2idx[n[1]] = count\n",
        "              count += 1\n",
        "              cur = n[1]\n",
        "        return labels, label2idx\n",
        "\n",
        "    def __getitem__(self, n: int) -> Tuple[Tensor, int]:\n",
        "        \"\"\"Load the n-th sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            n (int): The index of the sample to be loaded\n",
        "\n",
        "        Returns:\n",
        "            X (tensor): features tensor\n",
        "            Y (int): label id\n",
        "        \"\"\"\n",
        "        filepath = self.samples[n][0]\n",
        "        waveform, sample_rate, label = self._load_item(filepath, self.root_path)\n",
        "\n",
        "        # Transformations\n",
        "        frame_step = int(sample_rate * step_length) # 328\n",
        "        spec = torch.stft(waveform,           # spec.shape = (1, freq_bin, audio_len)\n",
        "                          n_fft=frame_length,\n",
        "                          hop_length=frame_step,\n",
        "                          return_complex=True)\n",
        "        resize = torchvision.transforms.Resize((32, 128))\n",
        "        X = torch.transpose(spec, 1, 2) # X.shape = (1, audio_len, freq_bin)\n",
        "        X = torch.real(X)\n",
        "        X = torch.abs(X)\n",
        "\n",
        "        # Get multiple parts of size (image_width, n_fft)\n",
        "        audio_len= spec.shape[2]\n",
        "        if audio_len > image_width:\n",
        "            partsCount = audio_len // image_width\n",
        "            parts = torch.zeros((partsCount, image_width, int(n_fft/2+1)))\n",
        "            # print('parts', parts.shape)\n",
        "            for i in range(partsCount):\n",
        "                p = i * image_width\n",
        "                # print(X[:,p:p+image_width,:].shape, p+image_width)\n",
        "                parts[i] = X[:,p:p+image_width,:]\n",
        "            # print('parts', parts.shape)\n",
        "        else:\n",
        "            parts = X\n",
        "\n",
        "        parts = resize(parts)\n",
        "        Y = self.label2idx[label]\n",
        "\n",
        "        return parts, Y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = ROOT_DIR + \"/test_data/vox\"\n",
        "dataset = SpeakerData(DATA_PATH, 5, ['id10182','id10634','id10820','id11004','id11232'])\n",
        "title = \"\"\n",
        "for speaker in dataset.labels:\n",
        "    title += speaker + \"\\n\"\n",
        "print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSMI4O19dKSI",
        "outputId": "c2b30583-d725-4bb2-8674-846e92749a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id10182\n",
            "id10634\n",
            "id10820\n",
            "id11004\n",
            "id11232\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehMDqOK3hj3W",
        "outputId": "ce5fdb38-1aec-418e-82b3-fbfddcd318c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "5H1WILqzvDFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKA79of5EUir"
      },
      "outputs": [],
      "source": [
        "class SpeakerCNN(nn.Module):\n",
        "    ''' Models a simple Convolutional Neural Network'''\n",
        "    def __init__(self, num_class):\n",
        "      ''' initialize the network '''\n",
        "      super(SpeakerCNN, self).__init__()\n",
        "      # 1 input image channel, 8 output channels,\n",
        "      # 3x3 square convolution kernel\n",
        "      self.conv1 = nn.Conv2d(1, 8, 3, padding='same')\n",
        "      self.conv2 = nn.Conv2d(8, 16, 3, padding='same')\n",
        "\n",
        "      self.conv3 = nn.Conv2d(16, 16, 3, padding='same')\n",
        "      self.conv4 = nn.Conv2d(16, 32, 3, padding='same')\n",
        "\n",
        "      self.conv5 = nn.Conv2d(32, 32, 3, padding='same')\n",
        "      self.conv6 = nn.Conv2d(32, 64, 3, padding='same')\n",
        "\n",
        "      # Max pooling over a (2, 2) window\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      # Average pooling over a (3, 3) window\n",
        "      self.avgpool = nn.AvgPool2d(3, 3)\n",
        "\n",
        "      self.fc1 = nn.Linear(64 * 10 * 2, 16)# 3x3 from image dimension\n",
        "      self.fc2 = nn.Linear(16, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "      ''' the forward propagation algorithm '''\n",
        "      x = self.pool(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "      x = self.pool(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "      x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "      x = self.avgpool(x)\n",
        "      x = x.view(-1, 64 * 10 * 2)\n",
        "      x = self.fc1(x)\n",
        "      x = torch.sigmoid(self.fc2(x))\n",
        "      return x\n",
        "\n",
        "model_acc = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "x_gMVH1zvM76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vw4lZyHdsLy"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = ROOT_DIR + \"/test_data/leaderspeech\"\n",
        "DATA_PATH = ROOT_DIR + \"/test_data/vox\"\n",
        "# DATA_PATH = ROOT_DIR + \"/test_data/librispeech/dev-clean\"\n",
        "\n",
        "# Hyper-parameters\n",
        "train_test_ratio = 0.8\n",
        "num_classes = 20   # 395 in vox, 40 in librispeech, 5 in leaderspeech\n",
        "batch_size = 1\n",
        "learning_rate = 0.00001\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Import data\n",
        "dataset = SpeakerData(DATA_PATH, num_classes)\n",
        "train_size = int(train_test_ratio * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Save title for training figure\n",
        "title = \"\"\n",
        "for speaker in dataset.labels:\n",
        "    title += speaker + \"\\n\"\n",
        "# print(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ7h93Vaz_4G"
      },
      "outputs": [],
      "source": [
        "model = SpeakerCNN(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__nj24Tmszg9"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfx-hVbvdmvJ"
      },
      "outputs": [],
      "source": [
        "num_epochs = 25\n",
        "loss_plt = []\n",
        "acc_plt = []\n",
        "for epoch in range(num_epochs):\n",
        "    loss_per_epoch = 0.0\n",
        "    correct = 0.0\n",
        "    running_loss = 0.0\n",
        "    steps = 0\n",
        "    total_steps = 0\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        for j in range(images.shape[1]):\n",
        "            single_image = images[:,j:j + 1,:,:]\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(single_image)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print data (CNN)\n",
        "            loss_per_epoch += loss.item()\n",
        "            running_loss += loss.item()\n",
        "        total_steps += images.shape[1]\n",
        "        steps += images.shape[1]\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / steps:.5f}')\n",
        "            steps = 0\n",
        "            running_loss = 0.0\n",
        "\n",
        "    loss_plt.append(round((100 * loss_per_epoch/total_steps), 5))\n",
        "    acc_plt.append(round((100 * correct/total_steps), 5))\n",
        "\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot loss and accuracy graph"
      ],
      "metadata": {
        "id": "4pw5erfWv2Qe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "973Y4GsUeup0"
      },
      "outputs": [],
      "source": [
        "# Loss graph\n",
        "plt.figure(1)\n",
        "fig, axs = plt.subplots()\n",
        "plt.plot(loss_plt)\n",
        "title = 'Training loss on {} speakers'.format(len(dataset.labels))\n",
        "axs.set_title(title)\n",
        "axs.set_ylabel('Loss')\n",
        "axs.set_xlabel('Epoch')\n",
        "plt.savefig('Loss: '+ title)\n",
        "# Accuracy graph\n",
        "plt.figure(2)\n",
        "fig, axs = plt.subplots()\n",
        "plt.plot(acc_plt, color='orange')\n",
        "title = 'Training accuracy on {} speakers'.format(len(dataset.labels))\n",
        "axs.set_title(title)\n",
        "axs.set_ylabel('Accuracy')\n",
        "axs.set_xlabel('Epoch')\n",
        "plt.savefig('Accuracy: '+ title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq9RA8fLXr9q"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-Ii7_8Ucjir"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model = SpeakerCNN(num_classes)\n",
        "model.load_state_dict(torch.load('model.ckpt'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        labels = labels.to(device)\n",
        "        images = images.to(device)\n",
        "\n",
        "        for j in range(images.shape[1]):\n",
        "            single_image = images[:,j:j + 1,:,:]\n",
        "            outputs = model(single_image)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            acc = 100 * correct / total\n",
        "\n",
        "    print('Test Accuracy of the model on the {} test images: {} %'.format(len(test_loader), acc))\n",
        "# model_acc.append(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix"
      ],
      "metadata": {
        "id": "5dDXKGFw0O1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io1TqbwKqwQH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "model = SpeakerCNN(num_classes)\n",
        "model.load_state_dict(torch.load('model.ckpt'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for images, labels in test_loader:\n",
        "    labels = labels.to(device)\n",
        "    images = images.to(device)\n",
        "\n",
        "    for j in range(images.shape[1]):\n",
        "        single_image = images[:,j:j + 1,:,:]\n",
        "        output = model(single_image) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "\n",
        "        labels = labels.data.cpu()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in dataset.labels],\n",
        "                     columns = [i for i in dataset.labels])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('confusion_matrix.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_pZqR6gbjd4A"
      ],
      "authorship_tag": "ABX9TyOBX9eOO/CrJjo2Nsgs94ru"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}