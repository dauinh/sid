{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR2Rvs6NVAuS"
      },
      "source": [
        "# Speaker Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUsmqYZIgdc8"
      },
      "source": [
        "## Installation & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U3U7NbTCXS6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9566b746-0196-47fc-ed8d-63223d7e6c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.25.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.59.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-io==0.25.0 in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.25.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-io==0.25.0) (0.25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorflow==2.8.0\n",
        "!pip3 install tensorflow-io==0.25.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL3EdaUOhGUv"
      },
      "source": [
        "### Project imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwG9JJB6U3OF",
        "outputId": "62780642-abc5-4211-df1a-d8324c076e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.0\n",
            "TensorFlow IO version: 0.25.0\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"TensorFlow IO version:\", tfio.__version__)\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(tf.executing_eagerly())\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-x6YLgMwXYmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18396b8d-c81a-49a4-8c32-81e4f92e873c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT_DIR='/content/drive/MyDrive/College/Research/Linh_2023_Research'\n",
        "\n",
        "DATASET_PATH=ROOT_DIR+'/test_data/vox'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsF8O-L0VRZA"
      },
      "source": [
        "## Data Processing\n",
        "[Keras Speaker Recognition](https://keras.io/examples/audio/speaker_recognition_using_cnn/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CXc8U7QY8Jwn"
      },
      "outputs": [],
      "source": [
        "SAMPLING_RATE = 16000\n",
        "\n",
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
        "    return audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FFT & MFCC Pre-processing"
      ],
      "metadata": {
        "id": "Ogn_zBISnt9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_MFCC = 13\n",
        "\n",
        "def audio_to_fft(audio):\n",
        "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
        "    # we need to squeeze the dimensions and then expand them again\n",
        "    # after FFT\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    # Return the absolute value of the first half of the FFT\n",
        "    # which represents the positive frequencies\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
        "\n",
        "def audio_to_mfcc(audio):\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # Convert the audio to MFCC\n",
        "    stfts = tf.signal.stft(audio, frame_length=1024, frame_step=256, fft_length=1024)\n",
        "    spectrograms = tf.abs(stfts)\n",
        "\n",
        "    # Warp the linear scale spectrograms into the mel-scale\n",
        "    num_spectrogram_bins = stfts.shape[-1]\n",
        "    lower_edge_hertz, upper_edge_hertz = 80.0, 7600.0\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "        N_MFCC, num_spectrogram_bins, SAMPLING_RATE, lower_edge_hertz, upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
        "\n",
        "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms\n",
        "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "    # Compute MFCCs from log_mel_spectrograms and take the first N_MFCC\n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :N_MFCC]\n",
        "    print(mfccs.shape)\n",
        "\n",
        "    return mfccs"
      ],
      "metadata": {
        "id": "4B5UlsIgnqTH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8WW3Ke_gny_"
      },
      "source": [
        "### [VGGish Embedding Colab](https://colab.research.google.com/drive/1E3CaPAqCai9P9QhJ3WYPNCVmrJU4lAhF)\n",
        "\n",
        "Tensorflow implementation of VGGish model takes `numpy` array as input. While this is possible in eager mode (default mode of Tensorflow 2.x), we cannot run VGGish model in a dataset mapping function as mapping functions do not run in eager model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy\n",
        "!pip install resampy tensorflow\n",
        "!pip install tf_slim\n",
        "!rm -rf models\n",
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "metadata": {
        "id": "I4h29FcBHvwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEWOcr68gjyZ",
        "outputId": "cee1f527-a163-4f57-9858-839483c16f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Check to see where are in the kernel's file system.\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiVWsKdzgmQs",
        "outputId": "b84bbf5b-ae63-4073-c5e7-725b1db49396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  277M  100  277M    0     0  98.8M      0  0:00:02  0:00:02 --:--:-- 98.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 73020  100 73020    0     0   156k      0 --:--:-- --:--:-- --:--:--  156k\n"
          ]
        }
      ],
      "source": [
        "# Grab the VGGish model\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwfNMNu6g7RI",
        "outputId": "274e4ccd-bf70-486b-f074-0311e153ee69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t\t requirements.txt\t   vggish_model.ckpt\t  vggish_smoke_test.py\n",
            "mel_features.py  sample_data\t\t   vggish_params.py\t  vggish_train_demo.py\n",
            "models\t\t vggish_export_tfhub.py    vggish_pca_params.npz\n",
            "__pycache__\t vggish_inference_demo.py  vggish_postprocess.py\n",
            "README.md\t vggish_input.py\t   vggish_slim.py\n"
          ]
        }
      ],
      "source": [
        "# Make sure we got the model data.\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnYEND0vgSkK",
        "outputId": "6d9dd1d4-0fa0-462a-ada9-bf580f4bcb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mel_features.py   vggish_export_tfhub.py    vggish_params.py\t   vggish_smoke_test.py\n",
            "README.md\t  vggish_inference_demo.py  vggish_postprocess.py  vggish_train_demo.py\n",
            "requirements.txt  vggish_input.py\t    vggish_slim.py\n"
          ]
        }
      ],
      "source": [
        "# Verify the location of the AudioSet source files\n",
        "!ls models/research/audioset/vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6BgFFjCg_7u"
      },
      "outputs": [],
      "source": [
        "# Copy the source files to the current directory.\n",
        "!cp models/research/audioset/vggish/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR6DF8CJhJri",
        "outputId": "35afd681-95da-4b5c-f3ef-555968edf4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t\t requirements.txt\t   vggish_model.ckpt\t  vggish_smoke_test.py\n",
            "mel_features.py  sample_data\t\t   vggish_params.py\t  vggish_train_demo.py\n",
            "models\t\t vggish_export_tfhub.py    vggish_pca_params.npz\n",
            "__pycache__\t vggish_inference_demo.py  vggish_postprocess.py\n",
            "README.md\t vggish_input.py\t   vggish_slim.py\n"
          ]
        }
      ],
      "source": [
        "# Make sure the source files got copied correctly.\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSrTZluHhMQ2"
      },
      "outputs": [],
      "source": [
        "# Run the test, which also loads all the necessary functions.\n",
        "from vggish_smoke_test import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vggish_slim\n",
        "import vggish_params\n",
        "import vggish_input\n",
        "\n",
        "def create_vggish_net(sess, hop_size=0.96):   # Hop size is in seconds.\n",
        "    \"\"\"Define VGGish model, load the checkpoint, and return a dictionary that points\n",
        "    to the different tensors defined by the model.\n",
        "    \"\"\"\n",
        "    vggish_slim.define_vggish_slim()\n",
        "    checkpoint_path = 'vggish_model.ckpt'\n",
        "    vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
        "\n",
        "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "    features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
        "    embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
        "\n",
        "    layers = {'conv1': 'vggish/conv1/Relu',\n",
        "            'pool1': 'vggish/pool1/MaxPool',\n",
        "            'conv2': 'vggish/conv2/Relu',\n",
        "            'pool2': 'vggish/pool2/MaxPool',\n",
        "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
        "            'pool3': 'vggish/pool3/MaxPool',\n",
        "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
        "            'pool4': 'vggish/pool4/MaxPool',\n",
        "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
        "            # 'fc2': 'vggish/fc2/Relu',\n",
        "            'embedding': 'vggish/embedding',\n",
        "            'features': 'vggish/input_features',\n",
        "            }\n",
        "    g = tf.compat.v1.get_default_graph()\n",
        "    for k in layers:\n",
        "        layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
        "\n",
        "    return {'features': features_tensor,\n",
        "            'embedding': embedding_tensor,\n",
        "            'layers': layers,\n",
        "            }\n",
        "\n",
        "@tf.function\n",
        "def embeddings_from_vggish(sess, vgg, x, sr=SAMPLING_RATE):\n",
        "    \"\"\"Run the VGGish model, starting with a sound (x) at sample rate\n",
        "    (sr). Return a dictionary of embeddings from the different layers\n",
        "    of the model.\"\"\"\n",
        "    # Produce a batch of log mel spectrogram examples.\n",
        "    def waveform_to_examples_py(x_np, sr):\n",
        "        x_np = np.array(x_np)\n",
        "        return vggish_input.waveform_to_examples(x_np, sr)\n",
        "\n",
        "    # Use tf.py_function to execute the Python function\n",
        "    input_batch = tf.py_function(waveform_to_examples_py, [x, sr], tf.float32)\n",
        "\n",
        "    layer_names = vgg['layers'].keys()\n",
        "    tensors = [vgg['layers'][k] for k in layer_names]\n",
        "\n",
        "    def concrete_func(x, sr):\n",
        "        return sess.run(tensors, feed_dict={vgg['features']: x})\n",
        "\n",
        "    # Execute the concrete function with the input tensor evaluated\n",
        "    results = concrete_func(input_batch, sr)\n",
        "\n",
        "    resdict = {}\n",
        "    for i, k in enumerate(layer_names):\n",
        "        resdict[k] = results[i]\n",
        "\n",
        "    embedding = tf.convert_to_tensor(resdict['embedding'])\n",
        "    return embedding\n",
        "\n",
        "# Define the graph and load the model\n",
        "with tf.Graph().as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "    vgg = create_vggish_net(sess, 0.01)\n",
        "\n",
        "# Iterate through dataset\n",
        "# ...\n",
        "\n",
        "# Close the session when done\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "I0zh1PH3n88M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpeechIdentity"
      ],
      "metadata": {
        "id": "FL_BvU0dIY3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/douglas125/SpeechIdentity.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG-m3hHeIbSY",
        "outputId": "f66f67a7-3e49-40dd-bfea-1136f2cb43f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SpeechIdentity'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 55 (delta 11), reused 23 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (55/55), 8.66 MiB | 1.60 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/SpeechIdentity/speech-id-model-110/ .\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq1HerVIItyL",
        "outputId": "aef6c2ed-ddbe-4ff3-8d2e-937590715113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  SpeechIdentity  speech-id-model-110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import train_speech_id_model\n",
        "\n",
        "model = tf.keras.models.load_model('speech-id-model-110')\n",
        "cur_data = tfio.audio.AudioIOTensor(file)\n",
        "audio_data = cur_data.to_tensor()[:, 0]\n",
        "# set batch size to 1, extract first element\n",
        "audio_embedding = model.predict(\n",
        "\ttf.expand_dims(audio_data, axis=0)\n",
        ")[0]"
      ],
      "metadata": {
        "id": "uUap9O53HSb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpeechT5"
      ],
      "metadata": {
        "id": "VRPjNtbEf-H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "LUkuQE0-gZI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cULOPqmmlTvD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForSpeechToText\n",
        "\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_asr\")\n",
        "model = SpeechT5ForSpeechToText.from_pretrained(\"microsoft/speecht5_asr\")"
      ],
      "metadata": {
        "id": "IKabrHj-gBNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation"
      ],
      "metadata": {
        "id": "AUfHLUxcn7NS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset"
      ],
      "metadata": {
        "id": "ddNperMNvrWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSzqjnpz4uhM",
        "outputId": "ce32aecc-3327-487f-a308-72b8f755ec50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing speaker id10004\n",
            "Processing speaker id10836\n",
            "Processing speaker id10964\n",
            "Processing speaker id10365\n",
            "Processing speaker id10022\n",
            "Found 699 files belonging to 395 classes.\n",
            "Using 560 files for training.\n",
            "Using 139 files for validation.\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_SEED = 43\n",
        "TRAIN_VALID_SPLIT = 0.2\n",
        "EPOCHS = 100\n",
        "N_CLASS = 5\n",
        "\n",
        "class_names = os.listdir(DATASET_PATH)\n",
        "random.shuffle(class_names)\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    if label > N_CLASS - 1: break\n",
        "    print(\"Processing speaker {}\".format(name,))\n",
        "    dir_path = Path(DATASET_PATH) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Split into training and validation\n",
        "num_val_samples = int(TRAIN_VALID_SPLIT * len(audio_paths))\n",
        "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "print(\"Using {} files for validation.\".format(num_val_samples))\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]\n",
        "\n",
        "# Create 2 datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data transformation"
      ],
      "metadata": {
        "id": "ZkJ3BqwLvt34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "VBfBOqArvl6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization (MFCC)"
      ],
      "metadata": {
        "id": "AB82g1OeGw2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frTFswK4CGRr"
      },
      "outputs": [],
      "source": [
        "# for i, data in enumerate(train_ds):\n",
        "#     if i > 0: break\n",
        "#     x, y = data\n",
        "#     # Extract the chosen sample\n",
        "#     selected_sample_np = x[i].numpy()\n",
        "\n",
        "#     # Display the MFCC for the selected sample\n",
        "#     plt.imshow(selected_sample_np, cmap='viridis', origin='lower', aspect='auto')\n",
        "#     plt.title(f'MFCC for Sample {i}')\n",
        "#     plt.xlabel('MFCC Coefficient')\n",
        "#     plt.ylabel('Time Step')\n",
        "#     plt.colorbar(label='Magnitude')\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jlin8sbJRYj"
      },
      "source": [
        "## Model\n",
        "\n",
        "MFCC\n",
        "\n",
        "FFT (focus on low freq) ---> CNN (max pool in one direction)\n",
        "\n",
        "Is speaker unique in consonant or vowel?\n",
        "\n",
        "To try: Cifar, Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyhOcSlQeYdw"
      },
      "source": [
        "### Simple ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imt11oYweVzh"
      },
      "outputs": [],
      "source": [
        "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
        "    # Shortcut\n",
        "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
        "    for i in range(conv_num - 1):\n",
        "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
        "        x = keras.layers.Activation(activation)(x)\n",
        "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
        "    x = keras.layers.Add()([x, s])\n",
        "    x = keras.layers.Activation(activation)(x)\n",
        "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "\n",
        "def simple_resnet(input_shape, num_classes):\n",
        "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
        "\n",
        "    x = residual_block(inputs, 16, 2)\n",
        "    x = residual_block(x, 32, 2)\n",
        "    x = residual_block(x, 64, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "    x = residual_block(x, 128, 3)\n",
        "\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    # x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
        "\n",
        "    return keras.models.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnFLlceDEwmQ"
      },
      "outputs": [],
      "source": [
        "# model = simple_resnet((SAMPLING_RATE//2, 1), N_CLASS)\n",
        "model = simple_resnet((59, N_MFCC), N_CLASS)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model using Adam's default learning rate\n",
        "model.compile(\n",
        "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Add callbacks:\n",
        "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
        "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
        "model_save_filename = \"model.h5\"\n",
        "\n",
        "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpMTXaQmJlRo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRecLUo_ELVz"
      },
      "source": [
        "`fit()` is for training the model with the given inputs (and corresponding training labels).\n",
        "\n",
        "`evaluate()` is for evaluating the already trained model using the validation (or test) data and the corresponding labels. Returns the loss value and metrics values for the model.\n",
        "\n",
        "`predict()` is for the actual prediction. It generates output predictions for the input samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33HkHl5aJnO2"
      },
      "outputs": [],
      "source": [
        "# Define the Keras TensorBoard callback\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb, tensorboard_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUHcoTI4j1en"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-Lf3hsLCZB"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYufHcUMLD_C"
      },
      "outputs": [],
      "source": [
        "print(model.evaluate(valid_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fsVZtGsEZ8z"
      },
      "outputs": [],
      "source": [
        "print(model.predict(valid_ds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2PB01p/UkqOa9w19ZxEfc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}